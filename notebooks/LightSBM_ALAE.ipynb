{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d549d0cb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../ALAE\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import deeplake\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from alae_ffhq_inference import load_model, encode, decode\n",
    "\n",
    "from notebooks_utils import TensorSampler\n",
    "\n",
    "\n",
    "from src.lightsbm import LightSBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38ee80",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca2628",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dim = 512\n",
    "\n",
    "input_data = 'ADULT'\n",
    "target_data = 'CHILDREN'\n",
    "\n",
    "output_seed = 42\n",
    "batch_size = 128\n",
    "eps = 0.1\n",
    "lr = 1e-3\n",
    "\n",
    "n_potentials = 10\n",
    "is_diag = True\n",
    "S_init = 0.1\n",
    "\n",
    "max_iter = 20000\n",
    "device = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207a09a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('../data'):\n",
    "    os.makedirs('../data')\n",
    "\n",
    "urls = {\n",
    "    \"../data/age.npy\": \"https://drive.google.com/uc?id=1Vi6NzxCsS23GBNq48E-97Z9UuIuNaxPJ\",\n",
    "    \"../data/gender.npy\": \"https://drive.google.com/uc?id=1SEdsmQGL3mOok1CPTBEfc_O1750fGRtf\",\n",
    "    \"../data/latents.npy\": \"https://drive.google.com/uc?id=1ENhiTRsHtSjIjoRu1xYprcpNd8M9aVu8\",\n",
    "    \"../data/test_images.npy\": \"https://drive.google.com/uc?id=1SjBWWlPjq-dxX4kxzW-Zn3iUR3po8Z0i\",\n",
    "}\n",
    "\n",
    "for name, url in urls.items():\n",
    "    gdown.download(url, os.path.join(f\"{name}\"), quiet=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "latents = np.load(\"../data/latents.npy\")\n",
    "gender = np.load(\"../data/gender.npy\")\n",
    "age = np.load(\"../data/age.npy\")\n",
    "test_inp_images = np.load(\"../data/test_images.npy\")\n",
    "\n",
    "train_latents, test_latents = latents[:train_size], latents[train_size:]\n",
    "train_gender, test_gender = gender[:train_size], gender[train_size:]\n",
    "train_age, test_age = age[:train_size], age[train_size:]\n",
    "\n",
    "if input_data == \"MAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif input_data == \"WOMAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif input_data == \"ADULT\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif input_data == \"CHILDREN\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "x_data_train = train_latents[x_inds_train]\n",
    "x_data_test = test_latents[x_inds_test]\n",
    "\n",
    "if target_data == \"MAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif target_data == \"WOMAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif target_data == \"ADULT\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif target_data == \"CHILDREN\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "y_data_train = train_latents[y_inds_train]\n",
    "y_data_test = test_latents[y_inds_test]\n",
    "\n",
    "X_train = torch.tensor(x_data_train)\n",
    "Y_train = torch.tensor(y_data_train)\n",
    "\n",
    "X_test = torch.tensor(x_data_test)\n",
    "Y_test = torch.tensor(y_data_test)\n",
    "\n",
    "X_sampler = TensorSampler(X_train, device=\"cpu\")\n",
    "Y_sampler = TensorSampler(Y_train, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7dea1e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40361673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LightSBM(dim=dim, n_potentials=n_potentials, epsilon=eps, S_diagonal_init=S_init, is_diagonal=is_diag)\n",
    "\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81c8ee",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9295daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, max_iter, eps, opt, val_freq=1000, batch_size=512, safe_t=1e-2, device=device):\n",
    "    \n",
    "    pbar = tqdm(range(1, max_iter + 1))\n",
    "    \n",
    "    for i in pbar:\n",
    "        \n",
    "        x_0_samples = X_sampler.sample(batch_size).to(device)      \n",
    "        x_1_samples = Y_sampler.sample(batch_size).to(device)\n",
    "        \n",
    "        t = torch.rand([batch_size, 1], device=device) * (1 - safe_t)\n",
    "        \n",
    "        x_t = x_1_samples * t + x_0_samples * (1 - t) + torch.sqrt(eps * t * (1 - t)) * torch.randn_like(x_0_samples)\n",
    "                \n",
    "        predicted_drift = model.get_drift(x_t, t.squeeze())\n",
    "        \n",
    "        loss_plan = (model.get_log_C(x_0_samples) - model.get_log_potential(x_1_samples)).mean()\n",
    "        \n",
    "        target_drift = (x_1_samples - x_t) / (1 - t)\n",
    "        \n",
    "        loss = F.mse_loss(target_drift, predicted_drift)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        pbar.set_description(f'Loss : {loss.item()} Plan Loss: {loss_plan.item()}')\n",
    "        \n",
    "        if wandb.run:\n",
    "            wandb.log({'loss_bm': loss, 'loss_plan': loss_plan})\n",
    "        \n",
    "        if i % val_freq == 0:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train(model, max_iter, eps, opt, val_freq=1000, batch_size=512, safe_t=1e-2, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fec04b",
   "metadata": {},
   "source": [
    "## Results Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alae_model = load_model(\"../ALAE/configs/ffhq.yaml\", training_artifacts_dir=\"../ALAE/training_artifacts/ffhq/\")\n",
    "torch.manual_seed(output_seed); np.random.seed(output_seed)\n",
    "\n",
    "inds_to_map = np.random.choice(np.arange((x_inds_test < 300).sum()), size=10, replace=False)\n",
    "number_of_samples = 3\n",
    "\n",
    "mapped_all = []\n",
    "latent_to_map = torch.tensor(test_latents[x_inds_test[inds_to_map]])\n",
    "\n",
    "inp_images = test_inp_images[x_inds_test[inds_to_map]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        mapped = model(latent_to_map.to(device))\n",
    "        mapped_all.append(mapped)\n",
    "    \n",
    "mapped = torch.stack(mapped_all, dim=1)\n",
    "\n",
    "decoded_all = []\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        decoded_img = decode(alae_model, mapped[:, k].cpu())\n",
    "        decoded_img = ((decoded_img * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).permute(0, 2, 3, 1).numpy()\n",
    "        decoded_all.append(decoded_img)\n",
    "        \n",
    "decoded_all = np.stack(decoded_all, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_pictures = 2\n",
    "\n",
    "fig, axes = plt.subplots(n_pictures, number_of_samples+1, figsize=(number_of_samples+1, n_pictures), dpi=200)\n",
    "\n",
    "for i, ind in enumerate(range(n_pictures)):\n",
    "    ax = axes[i]\n",
    "    ax[0].imshow(inp_images[ind])\n",
    "    for k in range(number_of_samples):\n",
    "        ax[k+1].imshow(decoded_all[ind, k])\n",
    "        \n",
    "        ax[k+1].get_xaxis().set_visible(False)\n",
    "        ax[k+1].set_yticks([])\n",
    "        \n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=0.05)\n",
    "fig.savefig('alae_transfer.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f143d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
